{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TXINE. Práctica3: Extracción de datos dunha rede social\n",
    "### Adolfo Núñez Fernández, Novembro 2015 \n",
    "## Requirimentos\n",
    "\n",
    "Extracción de datos dunha rede social para a súa posterior análise. Traballaremos con Reddit para construír unha colección de datos, que conteña envíos (posts e/ou comentarios) emitidos por usuarios en Reddit. Tentarase extraer o máximo número de posts e comentarios dunha das subcomunidades de Reddit (subreddits). Por exemplo \"science\", \"politics\", ... \n",
    "O programa, empregando as posibilidades da API de Reddit, definirá alomenos dous modos de extraer contidos: \n",
    "\n",
    "* os últimos contidos (extraendo o máximo número que permite Reddit) [NEW] \n",
    "* os contenidos máis populares (acorde ás valoraciones de Reddit) [TOP]\n",
    "* os contidos máis candentes [HOT]\n",
    "\n",
    "Este conxunto de entradas xunto cos seus comentarios asociados serán almacenados en disco en un formato XML. Este esquema XML permitirá almacenar toda a información dispoñible (alomenos, título, contido, data, tipo de entrada -post ou comentario-) nun único arquivo\n",
    "\n",
    "Finalmente realizarase un simple procesamento do corpus xerado no ficheiro XML para vectorizar a colección e amosar os termos con maior ponderación tf/idf. Para iso, filtrando stopwords e todas aquelas palabras que aparezan en menos de 10 documentos para vectorizar la colección e amosar os 10 termos máis \"centrais\" na colección, entendendo como máis centrais aqueles que teñan a maior suma acumulada de tf/idf sobre todos os documentos.\n",
    "\n",
    "## Explicación do código\n",
    "\n",
    "O programa estructúrase a partires dunha serie de menús e información requirida ao usuario. A función **main** ofrece un menú principal de opcións ao usuario, onde poderá escoller entre descargar e almacenar unha nova colección ou ler e analizar un arquivo XML almacenado previamente. No caso de escoller unha nova adquisición de datos, un novo menú permite escoller entre as tres opciósn posibles especificadas nos requisitos: **TOP/HOT/NEW**. Ademáis o usuario pode especificar o límite de documentos desexados (con opción de adquirir o máximo que permita **Reddit**) e asignar un nome ao ficheiro XML onde se gardará a colección. O programa dispón tamén dunha opción no código (comentada por defecto pola súa demora no tempo de execución), que permite a adquisición reiterada de comentarios sobre un post, e non só os que trae Reddit por defecto. \n",
    "\n",
    "Na segunda opción, o usuario especificará un ficheiro a ler e analizar. O programa automaticamente filtrará aquelas palabras  que aparezan en menos de 10 documentos e eliminará unha lista de stopwords, que son a unión dunha colección por defecto máis unha colección seleccionada tras a inspección experta de varias execucións sobre distintos subreddits. O programa amosará os 10 termos centrais da colección, tal como se soliciata nos requirimentos. O código está estruturado en funciósn independentes por funcionalidade, que se describen a continuación. \n",
    "\n",
    "### Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import praw\n",
    "import xml.etree.ElementTree as ET \n",
    "from datetime import datetime  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As librarías empregadas son:\n",
    "\n",
    "* **sys:** Para acceder ás excepcións do sistema.\n",
    "\n",
    "* **praw:** Para acceder á API de Reddit.\n",
    "\n",
    "* **xml.etree.ElementTree:** Permite manexar obxectos estructurados en forma de árbore. En concreto emprégase para a manipulación, escritura e lectura do ficheiro XML.\n",
    "\n",
    "* **datime:** Libraría estandar de python empregada para medir tempso de execución do programa.\n",
    "\n",
    "* **TfidfVectorizer:** Obxecto da libraria **sklearn** que permite a vectorización e análise de corpus de texto.\n",
    "\n",
    "### Main e funcións menu(), mainMenu()\n",
    "\n",
    "A función **main** permite a interacción do usuario co programa a través dun menú de tres opcións:\n",
    "* [1] Obtención de datos e gardado a XML\n",
    "* [2] Lectura de XML e análise Tfidf\n",
    "* [3] Saída do programa.\n",
    "\n",
    "O programa presenta o menú principal (**mainMenu()**) mentres non se escolle algunha das opcións válidas. \n",
    "\n",
    "Na opción 1, o programa conéctase á API de reddit mediante previa identificación a través dun **user_agent**. Establecida a conexión, o usuario debe escoller o límite de post a solicitar á API. Posteriormente preséntaselle un submenu (**menu()**) onde o usuario escollerá o tipo de posts que desexa:\n",
    "* [1] Hot: Máis candentes\n",
    "* [2] Top: Máis valorados\n",
    "* [3] New: Máis novos\n",
    "* [4] Cancelar: volta ao menú principal\n",
    "\n",
    "Finalmente deberá introducir o nome do ficheiro XML onde vai gardar a información. Se o usuario non proporciona ningún, automáticamente créase un por defectoco nome xerado polas variables: <TIPOPOST>_<nomeSubreddit>_<limite>. O programa continua coa chamada á función **data2XML()** que se encarga da recollida e almacenamento de datos.\n",
    "\n",
    "Na segunda opción, solicítaselle ao usuario o ficheiro a analizar, para proceder á súa lectura, análise e visualización de resultados, coas funcións **getCorpusFromXML()**, **vectorizaCorpus()** e **showResults** respectivamente. É aquí onde están definidas tamén as opcións de filtrado coma o número mínimo de documentos onde deben aparecer as palabras do corpus e o número de resultados a amosar tras a análise. \n",
    "\n",
    "A opción 3 remata a execución do programa, presentando unha mensaxe de despedida por pantalla. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mainMenu():\n",
    "    print(\"\\nEscolla unha opción:\\n\\\n",
    "    [1] Obtención de datos e gardado a XML\\n\\\n",
    "    [2] Lectura de XML e análise Tfidf\\n\\\n",
    "    [3] Saír\")\n",
    "    \n",
    "def menu():\n",
    "    print(\"\\nEscolla unha opción:\\n\\\n",
    "    [1] Hot\\n\\\n",
    "    [2] Top\\n\\\n",
    "    [3] New\\n\\\n",
    "    [4] Cancelar\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ''' Programa principal. Chamada ao menú de opcións '''\n",
    "    \n",
    "    mainOption = 0\n",
    "    while mainOption != 3:\n",
    "        \n",
    "        if mainOption == 1:\n",
    "            \n",
    "            # Opción inicial para que entre no bucle\n",
    "            opcion = 0\n",
    "              \n",
    "            while opcion != 4:\n",
    "                if opcion in (1,2,3):\n",
    "                    user_agent = \"windows:com.exemplo.socialnetworkdataextraction:v1.0 (by /u/rebuldeiro)\"\n",
    "                    r = praw.Reddit(user_agent=user_agent)\n",
    "                    #r.config.store_json_result = True\n",
    "                    \n",
    "                    limite = int(input('Límite de peticións (0 para maximo posible):'))\n",
    "                    numPetic = str(limite)                    \n",
    "                    if limite  == 0:\n",
    "                        limite = None\n",
    "                        numPetic = 'Max'\n",
    "                    try:\n",
    "                        subreddit = str(input('Comunidade (subreddit) a analizar: '))\n",
    "                        comunidade = r.get_subreddit(subreddit, fetch=True)\n",
    "                    except praw.errors.InvalidSubreddit:\n",
    "                        print('Non existe a comunidade {0}\\n'.format(subreddit))\n",
    "                        sys.exit()\n",
    "                        \n",
    "                if opcion == 1: # HOT\n",
    "                    print(\"Obtendo {0} posts {1} de {2}\".format(numPetic, 'HOT', comunidade))\n",
    "                    submissions = comunidade.get_hot(limit=limite) \n",
    "                    tipo = 'HOT'\n",
    "                    print('[FEITO]')\n",
    "                elif opcion == 2: # TOP\n",
    "                    print(\"Obtendo {0} posts {1} de {2}\".format(numPetic, 'TOP', comunidade))\n",
    "                    submissions = comunidade.get_top(limit=limite)\n",
    "                    tipo = 'TOP'                    \n",
    "                    print('[FEITO]')\n",
    "                elif opcion == 3: # NEW\n",
    "                    print(\"Obtendo {0} posts {1} de {2}\".format(numPetic, 'NEW', comunidade))\n",
    "                    submissions = comunidade.get_new(limit=limite)\n",
    "                    tipo = 'NEW'\n",
    "                    print('[FEITO]')\n",
    "                else:\n",
    "                    pass\n",
    "                # se a opción é unha da que nos interesa\n",
    "                if opcion in (1,2,3):\n",
    "                    fname = input('Nome de ficheiro XML a gardar (sen extensión): ')\n",
    "                    if fname:\n",
    "                        fname += '.xml'\n",
    "                    else:\n",
    "                        fname = '{0}_{1}_{2}.xml'.format(tipo, subreddit, limite)\n",
    "                    \n",
    "                    data2XML(submissions, fname)\n",
    "                    break\n",
    "                    \n",
    "                #chamada ao menú\n",
    "                menu()\n",
    "                # Recóllese a opción do usuario\n",
    "                opcion = int(input('Opción: '))\n",
    "            \n",
    "            mainOption = 0\n",
    "        \n",
    "        elif mainOption == 2:\n",
    "            try:\n",
    "                fxml = input('Nome de ficheiro (sen extensión): ') \n",
    "                corpus = getCorpusFromXML(fxml+'.xml')\n",
    "                minDf = 10\n",
    "                numResults = 10                \n",
    "                vectorizer, invVoc, sumaTfidf = vectorizaCorpus(corpus, minDf)\n",
    "                showResults(vectorizer, invVoc, sumaTfidf, numResults)\n",
    "            except IOError as e:\n",
    "                print(\"I/O error({0}): {1}\".format(e.errno,'Non se atopou o ficheiro'))\n",
    "            except:\n",
    "                print (\"Produciuse un erro inesperado:\", sys.exc_info()[0])\n",
    "                raise\n",
    "\n",
    "            mainOption = 0\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "        #chamada ao menú principal\n",
    "            mainMenu()\n",
    "            # Recóllese a opción do usuario\n",
    "            mainOption = int(input('Opción principal: '))        \n",
    "    if mainOption == 3:\n",
    "        print(\"Adeus\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función data2XML(posts, nomeFicheiro)\n",
    "\n",
    "**DEF:** Función que convirte as submissions (posts) obtidas a un ficheiro XML. \n",
    "\n",
    "**Parámetros de entarda:** \n",
    "* **posts:** é un iterable de obxectos tipo **submission**, que é a colección de submissions (posts e comentarios) obtido mediante a API de Reddit (https://praw.readthedocs.org/en/stable/)\n",
    "* **nomeFicheiro:** nome (ruta) do ficheiro co que se creará o XML onde se gardará a información.\n",
    "\n",
    "**Funcionamento:**\n",
    "\n",
    "Co módulo xml.etree.ElementTree (https://docs.python.org/3/library/xml.etree.elementtree.html)temos unha API para crear e parsear datos XML. A medida que percorremos as submissions imos creando elementos XML (etiquetas) engadíndoos á estructura. O que nos interesa para a nosa análise é unha estructura de textos. Crearemos unha estructura simple como a que se amosa a continuación, gardando a información relevante de cada post e cada comentario coma atributos da etiqueta XML correspondente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<Subreddit>\n",
    "    <Post atributosPost>TextoPost\n",
    "        <Comments>\n",
    "            <Comment atributosComment>Texto comment</comment>\n",
    "            <Comment atributosComment>Texto comment</comment>\n",
    "            <Comment atributosComment>Texto comment</comment>\n",
    "            ...\n",
    "            <Comment atributosComment>Texto comment</comment>\n",
    "        </Comments>\n",
    "    </Post>\n",
    "    <Post atributosPost>TextoPost\n",
    "    ...\n",
    "    </Post>\n",
    "</Subreddit>            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezaremos polo elemento raíz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rootNode = ET.Element('Subreddit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De cada obxecto submission podemos extraer os seus comentarios e comentarios de comentarios con:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments = praw.helpers.flatten_tree(submission.comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A estratexia que seguiremos será almacenar a información do obxecto submission (post) e comment en diccionario de atributos que lle pasaremos ás etiquetas XML que imos construíndo. O único filtrado qeu faremos será de eliminar aqueles comentarios que non teñen autor ou non teñen contido. Ao rematar con todas as submissions pechamos a conexión a Reddit e gardamos o ficheiro a XML gracias de novo ás utilidades do módulo ElementTree. O código da función comentado é:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data2XML(submissions, fname):\n",
    "    ''' Convirte as submissions obtidas a un ficheiro XML '''\n",
    "\n",
    "    startTime = datetime.now()    \n",
    "    rootNode = ET.Element('Subreddit')\n",
    "    rootNode.append(ET.Comment('Xerado para TXINE. Novembro 2015'))\n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for submission in submissions: # Cada submission é un post\n",
    "        try:\n",
    "            i += 1\n",
    "            print(\"Engadindo post {0} ao XML\\r\".format(i),end='')        \n",
    "            \n",
    "            '''\n",
    "            Descomentar só para obter máis comentarios (se existen) sobre o \n",
    "            post. Tarda máis: 10 posts ~ 13'30''  \n",
    "            '''  \n",
    "            #submission.replace_more_comments(limit=None, threshold=0)\n",
    "            \n",
    "            comments = praw.helpers.flatten_tree(submission.comments)\n",
    "            \n",
    "            # Gardamos os atributos que nos interesan de cada submission    \n",
    "            atrSub = {'author':submission.author.name,\n",
    "                      'date':str(submission.created),                  \n",
    "                      'date_utc':str(submission.created_utc),\n",
    "                      'id':str(submission.id),\n",
    "                      'num_total_comments':str(submission.num_comments), #inclue os eliminados\n",
    "                      'num_true_comments':str(len(comments)), #so comments e replys existentes\n",
    "                      'title':submission.title,\n",
    "                      'type':'post'\n",
    "                      }\n",
    "            elementoPost = ET.SubElement(rootNode, 'Post', atrSub)    \n",
    "            elementoPost.text = str(submission.title)+' : '+str(submission.selftext) # concatenase titulo e texto\n",
    "                             \n",
    "            if len(comments) > 0: #se hai comentarios\n",
    "                elementoComments = ET.SubElement(elementoPost,'Comments')\n",
    "                for comment in comments: # Percorremos os comentarios (non aparecen os eliminados) \n",
    "                    if hasattr(comment,'body') and hasattr(comment,'author') and str(comment.author) != 'None':\n",
    "                            atrCom = {'author': comment.author.name,\n",
    "                                      'date':str(comment.created),                  \n",
    "                                      'date_utc':str(comment.created_utc),\n",
    "                                      'id':str(comment.id),\n",
    "                                      'parent_id':str(comment.parent_id),\n",
    "                                      'type':'comment' if comment.is_root else 'reply'\n",
    "                                                }\n",
    "                            elementoComment = ET.SubElement(elementoComments,'Comment',atrCom)\n",
    "                            elementoComment.text = str(comment.body) \n",
    "        except Exception as e:\n",
    "            print('Exception: {0}'.format(e))    \n",
    "    r.http.close()\n",
    "    # Gardado a ficheiro\n",
    "    with open(fname, 'wb') as f:\n",
    "        ET.ElementTree(rootNode).write(f, method='xml')\n",
    "\n",
    "    print('\\nTempo de extracción e almacenamento: {0}'.format(datetime.now() - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función getCorpusFromXML(nomeFicheiro)\n",
    "\n",
    "**DEF:** Función encargada da obtención de información dun corpus textual a partires dun ficheiro XML.\n",
    "\n",
    "**Parámetros de entrada:** \n",
    "* **nomeFicheiro:** nome (ou path) do ficheiro XML\n",
    "\n",
    "**Parámetros de saída:**\n",
    "* **corpus** lista de textos\n",
    "\n",
    "**Funcionamento:**\n",
    "\n",
    "As funcionalidades de **elementTree** facilítannos a tarefa de recuperación dos textos dun ficheiro XML e almacenamento nunha lista. A función *parsea* o ficheiro a unha estructura de tipo árbore e a partires do seu nodo raíz par aposteriormente obter todos os textos correspondentes aos **posts** e aos **comentarios** en sendas listas, que unidas conformarán a lista **corpus** a devolver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCorpusFromXML(oXML):\n",
    "    # Lectura do ficheiro xml\n",
    "    tree = ET.parse(oXML)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Creamos un corpus para analizar con posts e comentarios \n",
    "    posts = [x.text for x in root.iter(\"Post\")]\n",
    "    comentarios = [x.text for x in root.iter(\"Comment\")]\n",
    "    \n",
    "    corpus = posts + comentarios\n",
    "    print('\\nCorpus creado con {0} posts e {1} comentarios'.format(len(posts), len(comentarios)))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Funcion vectorizaCorpus(corpus, minimoDocumentosAfiltrar)\n",
    "\n",
    "**DEF:** Función que se encarga da vectorización do corpus de texto para a súa posterior análise. \n",
    "\n",
    "**Parámetros de entrada:**\n",
    "* **corpus:** lista de documentos en formato texto\n",
    "* **minimoDocumentosAfiltrar:** enteiro que indica o número mínimo de documentos nos que debe aparecer unha palabra para que sexa considerada relevante para análise. \n",
    "\n",
    "**Parámetros de saída:**\n",
    "* **vectorizer:** vectorizador do corpus\n",
    "* **invVoc:** diccionario de índices-termos para a presentación de resultados \n",
    "* **sumaTfidfun:** lista onde se almacenan as suma acumulada do tf/idf en tódolos documentos, empregada coma medida de centralización. \n",
    "\n",
    "**Comportamento:**\n",
    "\n",
    "O primeiro que fai esta función é chamar a **TfidfVectorizer**, unha clase da libraría **scikit** (http://scikit-learn.org/stable/index.html) que serve para convertir unha colección de documentos (a nosa lista corpus) nunha matriz de características TF-IDF. Esta clase admite unha colección de *stopwords* para o idioma inglés, pero engadíronselle ademais unha serie de palabras recoñecidas coma irrelevantes mediante a observación dos resultados. Esta observación foi realizada sobre distintos subreddits, pois hai palabras de uso común que se repiten en moitos deles polo simple uso da linguaxe natural, e que non teñen que ver con palabras relevantes do tema a tratar. Algún exemplo poden ser palabras coma *thanks*, *yes*, *yeah*, *reddit*, *know*...\n",
    "\n",
    "Unha vez engadidas as stopwords, calculáse a matriz de documentos-termos a partires do corpus, e créase un vocabulario invertido para indexar os termos, de cara á súa presentación. Tamén se calcula a suma acumulada en tódolos documentos do tf/idf para a ordenación de resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizaCorpus(corpus, minDf):\n",
    "    ''' Vectoriza o corpus introducido filtrando as palabras que aparecen en \n",
    "    menos de minDf documentos'''\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(min_df = minDf, lowercase=True, stop_words='english')\n",
    "        \n",
    "        # Definimos unha lista propoia de stopwords\n",
    "        myStopwords = ['did','didn','does','doesn','don','just','isn', \\\n",
    "        'reddit', 'wasn','www','yeah','yes','like','able','thanks', \\\n",
    "        'know', 'think','ve', 'want','com','https','http',\\\n",
    "        'good', 'really', 'make', 'say', 'going', 'said', 'people','way', \\\n",
    "        'use']\n",
    "        \n",
    "        # engadimos as stop_words que queremos ao conxunto xa existente\n",
    "        vectorizer.stop_words = vectorizer.get_stop_words().union(myStopwords)\n",
    "        \n",
    "        # calculamos a matriz de documentos-términos\n",
    "        docTerms = vectorizer.fit_transform(corpus)\n",
    "        \n",
    "        # invertimos o vocabulario creando un diccionario de índices - termos\n",
    "        invVoc = {v: k for k, v in vectorizer.vocabulary_.items()} \n",
    "        \n",
    "        # buscamos os termos centrais, que son os que a suma acumulada de tf/idf en todos os documentos é maior\n",
    "        sumaTfidf = docTerms.sum(axis=0).tolist()[0] #calculamos a suma por columnas da matriz de documentos-termos\n",
    "    \n",
    "        return  vectorizer, invVoc, sumaTfidf\n",
    "    except Exception as e:\n",
    "        print('\\nOcorreu un problema: {0}'.format(e))\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion showResults(vectorizer, invVoc, sumaTfidf, numResults)\n",
    "\n",
    "**DEF:** Función que presenta os resultados ao usuario en forma de táboa por pantalla, segundo os requerimentos da práctica, é dicir, os 10 termos centrais, entendendo como máis centrais aqueles que teñan a maior suma acumulada de tf/idf sobre todos os documentos.\n",
    "\n",
    "**Parámetros de entrada:**\n",
    "* **vectorizer:** que posúe os valores idf para o corpus\n",
    "* **invVoc:** diccionario que relaciona índices e termos, para poder relacionar a orde na lista coas palabras do corpus\n",
    "* **sumaTfidf:** suma acumulada de tf/idf. Suma por columnas da matriz documentos-termos\n",
    "* **numResults:** número de resultados a amosar ao usuario. Está fixado a 10 nesta práctica.\n",
    "\n",
    "**Parámetros de saída:**\n",
    "* **Non hai**\n",
    "\n",
    "**Comportamento:**\n",
    "\n",
    "Cos parámetros de entrada crea unha lista e mediante a combinación das funcións **sorted** e **getKey** (esta última definida para indicar a columna pola que se ordeará a lista), conv´´irtea nunha lista ordenada en orde descendente. Finalmente amosa tantos resultados como vñan indicados polo parámetro *numResults*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showResults(vectorizer, invVoc, sumaTfidf, numResults):\n",
    "    # Almacenamos todo nunha lista para poder amosar os resultados ordeados\n",
    "    listaResultados = []\n",
    "    for i in range(len(sumaTfidf)):\n",
    "        listaResultados.append([invVoc[i], sumaTfidf[i], vectorizer.idf_[i]])\n",
    "    \n",
    "    def getKey(item):\n",
    "        ''' Función que devolve o numero de columna polo que imos ordear a lista '''\n",
    "        return item[1]\n",
    "    \n",
    "    listaResultadosOrdenada = sorted(listaResultados, key = getKey, reverse = True)\n",
    "    print('{:15s} {:>10s} {:>10s}\\n{:45s}'.format('Palabra','Suma','Idf','-'*45))\n",
    "    for i in range(numResults):\n",
    "        print('{:15s} {:10.2f} {:10.2f}'.format(listaResultadosOrdenada[i][0],\n",
    "                                     listaResultadosOrdenada[i][1],listaResultadosOrdenada[i][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  Exemplo de resultados:\n",
    "\n",
    "Amósanse a continuación catro saídas de resultados, con tantos resultados como foi posible obter. Dúas para ficheiros do tipo HOT e dúas para ficheiros de tipo TOP ambas sobre dous subreddits distintos: *politics* e *science*. \n",
    "\n",
    "Obsérvase que quizáis se podería ter metido coma *stopword* algun termo coma *time*, *right*, ... pero pode que nestes contextos sexa un termo relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Nome de ficheiro (sen extensión): TOP_science_None\n",
    "\n",
    "Corpus creado con 55 posts e 830 comentarios\n",
    "Palabra               Suma        Idf\n",
    "---------------------------------------------\n",
    "blood                41.05       3.28\n",
    "sex                  26.76       3.71\n",
    "time                 22.75       3.51\n",
    "depression           22.32       3.68\n",
    "pigeons              21.90       4.03\n",
    "study                18.82       4.07\n",
    "donate               17.74       4.12\n",
    "week                 15.81       4.03\n",
    "years                15.73       3.87\n",
    "inflammation         15.19       4.15\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------------\n",
    "Nome de ficheiro (sen extensión): HOT_science_None\n",
    "\n",
    "Corpus creado con 673 posts e 11272 comentarios\n",
    "Palabra               Suma        Idf\n",
    "---------------------------------------------\n",
    "time                145.38       3.48\n",
    "study               127.44       3.75\n",
    "water               117.79       4.28\n",
    "years               112.32       3.83\n",
    "science             110.05       4.08\n",
    "work                 98.28       3.93\n",
    "new                  94.31       4.06\n",
    "actually             94.12       3.95\n",
    "research             92.99       4.05\n",
    "article              87.83       4.19\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------------\n",
    "Nome de ficheiro (sen extensión): HOT_politics_None\n",
    "\n",
    "Corpus creado con 954 posts e 23668 comentarios\n",
    "Palabra               Suma        Idf\n",
    "---------------------------------------------\n",
    "trump               275.54       4.11\n",
    "right               259.29       3.76\n",
    "refugees            258.81       3.98\n",
    "sanders             209.94       4.19\n",
    "bernie              201.36       4.33\n",
    "time                191.01       4.06\n",
    "obama               189.39       4.33\n",
    "money               188.49       4.18\n",
    "vote                182.89       4.45\n",
    "clinton             182.35       4.37\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------------\n",
    "Nome de ficheiro (sen extensión): TOP_politics_None\n",
    "\n",
    "Corpus creado con 293 posts e 4978 comentarios\n",
    "Palabra               Suma        Idf\n",
    "---------------------------------------------\n",
    "trump               105.12       3.64\n",
    "right                75.39       3.68\n",
    "clinton              67.20       4.04\n",
    "bernie               63.27       4.12\n",
    "hillary              62.22       4.07\n",
    "sanders              59.54       4.06\n",
    "vote                 54.80       4.25\n",
    "obama                51.95       4.30\n",
    "government           47.98       4.18\n",
    "republican           46.13       4.36"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
